{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment with bigstream\n",
    "---\n",
    "\n",
    "BigStream is a library of tools for image registration of huge images. It uses big data tools like Zarr and DASK to enable working with too-large-for-memory datasets and to make costly alignments finish in practical amounts of time by distributing the work on your workstation or cluster.\n",
    "\n",
    "This tutorial will walk you through the following steps:\n",
    "\n",
    "    1. Reading image data and metadata using Zarr\n",
    "    2. Global affine alignment\n",
    "    3. Piecewise affine alignments\n",
    "    4. Piecewise deformable alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "---\n",
    "\n",
    "Make sure BigStream is installed: `pip install bigstream`\n",
    "\n",
    "You should also get the source code, which is located here: https://github.com/GFleishman/bigstream. \\\n",
    "Follow the instrucions on github to clone the repository, which contains the example data used for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial data\n",
    "---\n",
    "\n",
    "In the BigStream repository the `resources` folder contains two images in N5 format.\\\n",
    "We will first access the data and metadata in these files using Zarr, which was installed with BigStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for reading\n",
    "import zarr\n",
    "\n",
    "# file paths to tutorial data N5 files\n",
    "fix_path = '/groups/scicompsoft/home/fleishmang/source/bigstream/resources/fix.n5'\n",
    "mov_path = '/groups/scicompsoft/home/fleishmang/source/bigstream/resources/mov.n5'\n",
    "\n",
    "# create Zarr file object using N5Stores\n",
    "fix_zarr = zarr.open(store=zarr.N5Store(fix_path), mode='r')\n",
    "mov_zarr = zarr.open(store=zarr.N5Store(mov_path), mode='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fix_zarr` and `mov_zarr` are just lazy pointers to the N5 files; no image data has been loaded into memory yet.\\\n",
    "The first alignment step, global affine, only needs low resolution data; which we assume fits into available memory. \\\n",
    "You'll see later how to deal with images that do not fit into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need numpy now\n",
    "import numpy as np\n",
    "\n",
    "# get pointers to the low res scale level\n",
    "# still just pointers, no data loaded into memory yet\n",
    "fix_lowres = fix_zarr['/lowres']\n",
    "mov_lowres = mov_zarr['/lowres']\n",
    "\n",
    "# we need the voxel spacings for the low res data sets\n",
    "# we can compute them from the low res data set metadata\n",
    "fix_meta = fix_lowres.attrs.asdict()\n",
    "mov_meta = mov_lowres.attrs.asdict()\n",
    "fix_lowres_spacing = np.array(fix_meta['pixelResolution']) * fix_meta['downsamplingFactors']\n",
    "mov_lowres_spacing = np.array(mov_meta['pixelResolution']) * mov_meta['downsamplingFactors']\n",
    "\n",
    "# read data into memory as numpy arrays\n",
    "# Why transpose? Images were stored as zyx, but we prefer xyz (metadata is already xyz)\n",
    "fix_lowres_data = fix_lowres[...].transpose(2, 1, 0)\n",
    "mov_lowres_data = mov_lowres[...].transpose(2, 1, 0)\n",
    "\n",
    "# sanity check: print the voxel spacings and lowres dataset shapes\n",
    "print(fix_lowres_data.shape, fix_lowres_spacing)\n",
    "print(mov_lowres_data.shape, mov_lowres_spacing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global affine alignment\n",
    "---\n",
    "\n",
    "The affine alignment algorithm is composed of three steps:\n",
    "\n",
    "    1. Key point extraction from fixed and moving images\n",
    "    2. Correspondence matching between fixed and moving key points using neighborhood correlation\n",
    "    3. Affine alignment of corresponding key points using RANSAC\n",
    "\n",
    "But this is all accomplished with one function call. The following cell will take some time to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affine alignment functions are in bigstream.affine\n",
    "from bigstream import affine\n",
    "\n",
    "# see below for explanation of parameters\n",
    "global_affine = affine.ransac_affine(\n",
    "    fix_lowres_data, mov_lowres_data,\n",
    "    fix_lowres_spacing, mov_lowres_spacing,\n",
    "    min_radius=6, max_radius=20, match_threshold=0.75,\n",
    ")\n",
    "\n",
    "# sanity check: print the result\n",
    "print(global_affine)\n",
    "\n",
    "# For tutorial data, should be approximately:\n",
    "# [[ 9.91070543e-01  3.40205967e-02 -5.63125159e-03 -8.12590407e+01]\n",
    "#  [-3.86715664e-02  1.02023436e+00 -6.30453307e-03 -1.33468687e+01]\n",
    "#  [ 1.19321249e-02 -2.21371673e-02  9.67441910e-01 -2.42373194e-01]\n",
    "#  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and output\n",
    "\n",
    "    min_radius: radius in voxels of smallest expected blob/cell size\n",
    "    \n",
    "    max_radius: radius in voxels of largest expected blob/cell size\n",
    "    \n",
    "    match_threshold: neighborhood correlation between two key points must exceed this value for it to be a valid match\n",
    "    \n",
    "Other optional parameters are:\n",
    "    \n",
    "    cc_radius: key points are matched using correlation of the data in their neighborhoods, this is the neighborhood radius in voxels (default: 12)\n",
    "    \n",
    "    nspots: the maximum number of key point pairs to use to compute the affine alignment (default: 5000)\n",
    "    \n",
    "    align_threshold: points are considered aligned by the affine if they are less than this value apart, in micrometers (default: 2.0)\n",
    "    \n",
    "    num_sigma_max: the maximum number of filters to run in the blob detection (default: 10)\n",
    "    \n",
    "The return value is:\n",
    "    \n",
    "    global_affine: the return value is a 4x4 numpy array containing an affine transform matrix; this describes correspondence between points in the fixed image and moving image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying global affine\n",
    "\n",
    "The alignment only gave us the affine transform matrix. Here we apply it to the moving image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for applying transforms are in bigstream.transform\n",
    "from bigstream import transform\n",
    "\n",
    "# apply the global affine to the moving image\n",
    "mov_lowres_aligned = transform.apply_global_affine(\n",
    "    fix_lowres_data, mov_lowres_data,\n",
    "    fix_lowres_spacing, mov_lowres_spacing,\n",
    "    global_affine,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the fixed, moving, and aligned data. Try running this cell a few times with different values for `slc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll visualize the results with some image plots\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot some image slices to check on things\n",
    "slc = 70\n",
    "f_slc = fix_lowres_data[..., slc]\n",
    "a_slc = mov_lowres_aligned[..., slc]\n",
    "m_slc = mov_lowres_data[..., slc]\n",
    "\n",
    "# normalize for display\n",
    "f_slc = f_slc.astype(np.float32) / f_slc.max()\n",
    "a_slc = a_slc.astype(np.float32) / a_slc.max()\n",
    "m_slc = m_slc.astype(np.float32) / m_slc.max()\n",
    "\n",
    "# make RGB versions\n",
    "f_rgb = np.zeros(f_slc.shape + (3,))\n",
    "f_rgb[..., 0] = f_slc * 2\n",
    "a_rgb = np.zeros(a_slc.shape + (3,))\n",
    "a_rgb[..., 0] = f_slc * 2\n",
    "a_rgb[..., 1] = a_slc * 2\n",
    "m_rgb = np.zeros(m_slc.shape + (3,))\n",
    "m_rgb[..., 1] = m_slc * 2\n",
    "\n",
    "# create figure and subplots\n",
    "fig = plt.figure(figsize=(12,24))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(f_rgb)\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(f_rgb)\n",
    "plt.imshow(a_rgb)\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(m_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local affine alignments\n",
    "---\n",
    "\n",
    "The tutorial dataset is small, so the global affine lines up almost all of the data well. But with a real dataset we would likely need to refine the global alignment with local alignments. In local affine alignment, the images are carved into overlapping tiles and a separate affine is computed for each tile. For large data sets there can be many tiles. To make this process tractable a cluster is constructed using [ClusterWrap](https://github.com/GFleishman/ClusterWrap) and DASK and the local affines are all computed in parallel.\n",
    "\n",
    "\n",
    "The `prepare_piecewise_ransac_affine` function API is the same as `ransac_affine`, but with one new required argument:\n",
    "\n",
    "    blocksize: iterable, length equal to the image dimension. The size of tiles in voxels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note use of mov_lowres_aligned as moving image rather than mov_lowres_data\n",
    "# Note also that fix_lowres_spacing is used as the \"moving\" voxel spacing here\n",
    "local_affines = affine.prepare_piecewise_ransac_affine(\n",
    "    fix_lowres_data, mov_lowres_aligned,\n",
    "    fix_lowres_spacing, fix_lowres_spacing,\n",
    "    min_radius=6, max_radius=20, match_threshold=0.75,\n",
    "    blocksize=[128,]*3,\n",
    ")\n",
    "\n",
    "\n",
    "# not a numpy array\n",
    "print(type(local_affines))\n",
    "\n",
    "# the first three dimensions index over the tiles\n",
    "# the last two dimensions are the 4x4 affine matrices for each tile\n",
    "print(local_affines.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the function returned immediately and did not return a numpy array. Any function in `bigstream` that begins with `prepare_` does not actually compute anything and does not return a concrete result. Instead, it sets up all the necessary relationships between functions and data to compute the result _some time later_. You can think of `prepare_` functions as returning a plan with a promise to compute later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing a prepared computation\n",
    "---\n",
    "Prepared computations can be executed on your local workstation or on a cluster with the `execute` function. The two cases are nearly identical, but we'll go over them separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workstation\n",
    "(If you're working on a cluster you can skip this section)\n",
    "\n",
    "BigStream is designed to work with very large datasets and every workstation has different (limited) resources and usage burden. When executing on a workstation it's important to think about:\n",
    "\n",
    "    n_workers: the number of python processes to run in parallel\n",
    "    \n",
    "    threads_per_worker: the number of threads each process will run in parallel\n",
    "    \n",
    "    memory_limit: the maximum amount of RAM each worker can use\n",
    "    \n",
    "    config: changes to the dask configuration\n",
    "\n",
    "The default value of `None` will cause BigStream to determine its resource limits automatically and if necessary it will max out your machine. To limit BigStream's access `n_workers` and `threads_per_worker` should be set based on how many cpu cores your workstation has and what other work the computer will be doing alongside bigstream. `memory_limit` is a threshold that if any individual worker crosses, the worker gets shutdown. `config` changes should really only be necessary if you run into problems, but here is a [list of dask configuration settings](https://docs.dask.org/en/latest/configuration-reference.html).\n",
    "\n",
    "Basically, when doing big data computations, you need to be knowledgeable about your workstation's resources, the size of your data, and how to monitor resource consumption of an execution using `task manager`, `activity monitor`, `top` or similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for executing prepared computations\n",
    "from bigstream.cluster import execute\n",
    "\n",
    "# execute the prepared computation, specifying resource limitations\n",
    "local_affines = execute(\n",
    "    local_affines,\n",
    "    n_workers=None,\n",
    "    threads_per_worker=None,\n",
    "    memory_limit=None,\n",
    "    config={},\n",
    ")\n",
    "\n",
    "# now we have a numpy array\n",
    "print(type(local_affines))\n",
    "print(local_affines.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster\n",
    "(If you're working on a workstation you can skip this section)\n",
    "\n",
    "If your cluster manager is LSF then you're in luck, `bigstream` should work for you. If your cluster manager is not LSF, then don't worry. Adding support for additional cluster managers is a high priority and bigstream has been designed to make this very easy. Please [post an issue on the repository](https://github.com/GFleishman/bigstream/issues) and indicate which cluster manager you'd like support for.\n",
    "\n",
    "On a cluster you have practically unlimited resources. BigStream will dynamically allocate workers between specified limits based on the size of the computation you give it. The resource limits you specify impact your time-to-result and cost. \n",
    "\n",
    "    cores: the number of cpu cores each worker gets\n",
    "    \n",
    "    processes: the number of python processes to run on each worker\n",
    "    \n",
    "    min_workers: minimum number of workers to maintain\n",
    "    \n",
    "    max_workers: the maximum number of workers to maintain\n",
    "    \n",
    "    config: changes to the dask configuration\n",
    "\n",
    "BigStream will parallelize at the level of workers, processes, _and_ threads - so you have a lot of flexibility for setting up your computation. The default settings below are a good place to start. As your jobs get bigger you can increase things like `min_workers` and `max_workers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for executing prepared computations\n",
    "from bigstream.cluster import execute\n",
    "\n",
    "# execute the prepared computation, specifying resource limitations\n",
    "local_affines = execute(\n",
    "    local_affines,\n",
    "    cores=4,\n",
    "    processes=1,\n",
    "    min_workers=1,\n",
    "    max_workers=4,\n",
    "    config={},\n",
    ")\n",
    "\n",
    "# now we have a numpy array\n",
    "print(type(local_affines))\n",
    "print(local_affines.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying local affines\n",
    "---\n",
    "\n",
    "The `prepare_apply_local_affines` function API is the same as `apply_global_affine`, but with one new required argument and one new optional argument:\n",
    "\n",
    "    blocksize: iterable, length equal to the image dimension. The size of tiles in voxels. Must be the same as the blocksize used to learn the local affines.\n",
    "    \n",
    "    global_affine (optional): a global affine matrix to apply before the local affines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the local affines to the moving image\n",
    "#   Note we're using mov_lowres_data again - it's better\n",
    "#   to provide the global and local affines together. They\n",
    "#   are composed into a single transform - that way the moving\n",
    "#   image is only resampled one time.\n",
    "mov_lowres_aligned = transform.prepare_apply_local_affines(\n",
    "    fix_lowres_data, mov_lowres_data,\n",
    "    fix_lowres_spacing, mov_lowres_spacing,\n",
    "    local_affines,\n",
    "    blocksize=[128,]*3,\n",
    "    global_affine=global_affine,\n",
    ")\n",
    "\n",
    "# prepared computation, so not a numpy array yet\n",
    "print(type(mov_lowres_aligned))\n",
    "print(mov_lowres_aligned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute using defaults\n",
    "# CONSIDER if you need to change resource parameters as discussed in \"Executing a prepared compuation\" section above\n",
    "mov_lowres_aligned = execute(mov_lowres_aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to before, we can inspect the alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some image slices to check on things\n",
    "slc = 70\n",
    "f_slc = fix_lowres_data[..., slc]\n",
    "a_slc = mov_lowres_aligned[..., slc]\n",
    "m_slc = mov_lowres_data[..., slc]\n",
    "\n",
    "# normalize for display\n",
    "f_slc = f_slc.astype(np.float32) / f_slc.max()\n",
    "a_slc = a_slc.astype(np.float32) / a_slc.max()\n",
    "m_slc = m_slc.astype(np.float32) / m_slc.max()\n",
    "\n",
    "# make RGB versions\n",
    "f_rgb = np.zeros(f_slc.shape + (3,))\n",
    "f_rgb[..., 0] = f_slc * 2\n",
    "a_rgb = np.zeros(a_slc.shape + (3,))\n",
    "a_rgb[..., 0] = f_slc * 2\n",
    "a_rgb[..., 1] = a_slc * 2\n",
    "m_rgb = np.zeros(m_slc.shape + (3,))\n",
    "m_rgb[..., 1] = m_slc * 2\n",
    "\n",
    "# create figure and subplots\n",
    "fig = plt.figure(figsize=(12,24))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(f_rgb)\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(f_rgb)\n",
    "plt.imshow(a_rgb)\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(m_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with higher resolution data\n",
    "---\n",
    "\n",
    "The final step is deformable alignment which can take advantage of high resolution features. Here we assume the higher resolution data is too large for memory and/or would be intractably slow to work on without parallelization. This section shows how BigStream's use of distributed computing enables you to align your data despite of these difficulties.\n",
    "\n",
    "Compare the cell below to the second cell in the _Tutorial Data_ section. Note that here we do not read the data into memory but proceed with the lazy `zarr.Array` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pointers to the high res scale level\n",
    "# still just pointers, no data loaded into memory\n",
    "fix_highres = fix_zarr['/highres']\n",
    "mov_highres = mov_zarr['/highres']\n",
    "\n",
    "# we need the voxel spacings for the high res data sets\n",
    "# we can compute them from the high res data set metadata\n",
    "fix_meta = fix_highres.attrs.asdict()\n",
    "mov_meta = mov_highres.attrs.asdict()\n",
    "fix_highres_spacing = np.array(fix_meta['pixelResolution']) * fix_meta['downsamplingFactors']\n",
    "mov_highres_spacing = np.array(mov_meta['pixelResolution']) * mov_meta['downsamplingFactors']\n",
    "\n",
    "# sanity check: print the voxel spacings and lowres dataset shapes\n",
    "print(fix_highres_spacing, mov_highres_spacing)\n",
    "print(fix_highres.shape, mov_highres.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the voxel spacings are in xyz order but the lazy array shapes are still zyx.\n",
    "\n",
    "\n",
    "Before we can deform, we to apply the global and local affines to this high res version of the data. Compare the cell below to the first cell in the _Apply local affines_ subsection. First note we provide the highres lazy zarr.Arrays as inputs. We happen to know they are 2x larger along each axis - so importantly the `blocksize` parameter has been doubled. Also note we're using an additional optional parameter here:\n",
    "\n",
    "    transpose: List of 3 boolean values. Should we transpose the axis order of the fixed image, moving image, or transform data.\n",
    "\n",
    "In this case, since the fixed and moving image data are being read from zarr files, they must be transposed - but because the transform is being constructed from the global and local affine matrices, it does not need to be transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the affines to the highres moving image\n",
    "mov_highres_aligned = transform.prepare_apply_local_affines(\n",
    "    fix_highres, mov_highres,\n",
    "    fix_highres_spacing, mov_highres_spacing,\n",
    "    local_affines,\n",
    "    blocksize=[256,]*3,\n",
    "    global_affine=global_affine,\n",
    "    transpose=[True, True, False],\n",
    ")\n",
    "\n",
    "# sanity check\n",
    "print(type(mov_highres_aligned))\n",
    "print(mov_highres_aligned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time, we'll write the aligned data to disk\n",
    "write_path = './mov_highres_affine_aligned.zarr'\n",
    "\n",
    "# execute with defaults\n",
    "mov_highres_aligned = execute(mov_highres_aligned, write_path=write_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of `write_path` in `execute`. We've assumed the high resolution data is too large to fit into memory, so instead of computing the result and returning it to the memory of the local process, we're elected to write the result to disk as a zarr file and return a reference to the `zarr.Array` object.\n",
    "\n",
    "Ensure the application was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some image slices to check on things\n",
    "#   Note for highres data we doubled the slice number\n",
    "#   The fix/mov images still need to be transposed,\n",
    "#   But mov_highres_aligned, which we made in the previous step\n",
    "#   was written out in xyz order\n",
    "slc = 140\n",
    "f_slc = fix_highres[slc, ...].transpose(1,0)\n",
    "a_slc = mov_highres_aligned[..., slc]\n",
    "m_slc = mov_highres[slc, ...].transpose(1,0)\n",
    "\n",
    "# normalize for display\n",
    "f_slc = f_slc.astype(np.float32) / f_slc.max()\n",
    "a_slc = a_slc.astype(np.float32) / a_slc.max()\n",
    "m_slc = m_slc.astype(np.float32) / m_slc.max()\n",
    "\n",
    "# make RGB versions\n",
    "f_rgb = np.zeros(f_slc.shape + (3,))\n",
    "f_rgb[..., 0] = f_slc * 2\n",
    "a_rgb = np.zeros(a_slc.shape + (3,))\n",
    "a_rgb[..., 0] = f_slc * 2\n",
    "a_rgb[..., 1] = a_slc * 2\n",
    "m_rgb = np.zeros(m_slc.shape + (3,))\n",
    "m_rgb[..., 1] = m_slc * 2\n",
    "\n",
    "# create figure and subplots\n",
    "fig = plt.figure(figsize=(12,24))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(f_rgb)\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(f_rgb)\n",
    "plt.imshow(a_rgb)\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(m_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deformable alignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the mandatory inputs to `prepare_piecewise_deformable_align` play the same role as they did for affine alignment. There are many additional optional parameters that control the nature of the deformable alignment:\n",
    "\n",
    "    cc_radius: size of neighborhood around each voxel to compute correlations for image matching term (default 12)\n",
    "    \n",
    "    gradient_smoothing: list of length 4, e.g. [a, b, c, d], parameters to gradient field smoothing kernel:\n",
    "    (a * div(grad) + b * grad(div) + c)^d    default [3., 0., 1., 2.] (for now, b should always be 0)\n",
    "    \n",
    "    field_smoothing: list of length 4, e.g. [a, b, c, d], parameters to total field smoothing kernel:\n",
    "    (a * div(grad) + b * grad(div) + c)^d    default [0.5., 0., 1., 6.] (for now, b should always be 0)\n",
    "    \n",
    "    iterations: list, number of iterations at each scale level, default [200, 100]\n",
    "    \n",
    "    shrink_factors: list, subsampling factor for each scale level, default [2, 1]\n",
    "    \n",
    "    smooth_sigmas: list, Gaussian kernel widths for anti-aliasing filter when sub-sampling at each scale level, default [4., 2.]\n",
    "    \n",
    "    step: maximum length each gradient descent step can take in micrometers, default: minimum of fixed image voxel spacings\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deform functions are in bistream.deform\n",
    "from bigstream import deform\n",
    "\n",
    "# deform the moving image\n",
    "full_field = deform.prepare_piecewise_deformable_align(\n",
    "    fix_highres, mov_highres_aligned,\n",
    "    fix_highres_spacing, fix_highres_spacing,\n",
    "    blocksize=[256,]*3,\n",
    "    transpose=[True, False],\n",
    "    global_affine=global_affine,\n",
    "    local_affines=local_affines,\n",
    "    iterations=[2, 1],\n",
    ")\n",
    "\n",
    "# sanity check\n",
    "print(type(full_field))\n",
    "print(full_field.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a write location for the final transform\n",
    "write_path = './full_highres_transform.zarr'\n",
    "\n",
    "# execute with defaults\n",
    "full_field = execute(full_field, write_path=write_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the full transform\n",
    "---\n",
    "We provided `global_affine` and `local_affines` to the `prepare_piecewise_deformable_align` function, so the position field it returned includes those transforms. Therefore, `full_field` is the complete transform between the fixed and moving images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the local affines to the moving image\n",
    "mov_highres_aligned = transform.prepare_apply_position_field(\n",
    "    fix_highres, mov_highres,\n",
    "    fix_highres_spacing, mov_highres_spacing,\n",
    "    full_field,\n",
    "    blocksize=[256,]*3,\n",
    "    transpose=[True, True, False],\n",
    ")\n",
    "\n",
    "# sanity check\n",
    "print(type(mov_highres_aligned))\n",
    "print(mov_highres_aligned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a write path for the final result\n",
    "write_path = './mov_highres_deform_aligned.zarr'\n",
    "\n",
    "# execute using defaults\n",
    "mov_highres_aligned = execute(mov_highres_aligned, write_path=write_path, project='multifish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some image slices to check on things\n",
    "slc = 140\n",
    "f_slc = fix_highres[slc, ...].transpose(1,0)\n",
    "a_slc = mov_highres_aligned[..., slc]\n",
    "m_slc = mov_highres[slc, ...].transpose(1,0)\n",
    "\n",
    "# normalize for display\n",
    "f_slc = f_slc.astype(np.float32) / f_slc.max()\n",
    "a_slc = a_slc.astype(np.float32) / a_slc.max()\n",
    "m_slc = m_slc.astype(np.float32) / m_slc.max()\n",
    "\n",
    "# make RGB versions\n",
    "f_rgb = np.zeros(f_slc.shape + (3,))\n",
    "f_rgb[..., 0] = f_slc * 2\n",
    "a_rgb = np.zeros(a_slc.shape + (3,))\n",
    "a_rgb[..., 0] = f_slc * 2\n",
    "a_rgb[..., 1] = a_slc * 2\n",
    "m_rgb = np.zeros(m_slc.shape + (3,))\n",
    "m_rgb[..., 1] = m_slc * 2\n",
    "\n",
    "# create figure and subplots\n",
    "fig = plt.figure(figsize=(12,24))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(f_rgb)\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(f_rgb)\n",
    "plt.imshow(a_rgb)\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(m_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigstream_testing",
   "language": "python",
   "name": "bigstream_testing"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
