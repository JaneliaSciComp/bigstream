{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigstream Tutorial\n",
    "---\n",
    "\n",
    "Bigstream is an extremely powerful and customizable tool. This tutorial will only show a small piece of what bigstream can do, but it's a reasonable place to start. This notebook will show small image in-memory registration and large image bigger-than-memory registration for a small example dataset.\n",
    "\n",
    "Our example data is stored in .n5 format, which can be read with the zarr library. Zarr files themselve would also be perfectly fine. For the small image in-memory registration you simply need numpy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Inputs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules for loading/writing data\n",
    "import numpy as np\n",
    "import zarr, tifffile\n",
    "\n",
    "# file paths to tutorial data\n",
    "# replace the capitalized text below with the path to your copy of the bigstream repository\n",
    "fix_path = '/PATH/TO/BIGSTREAM/REPOSITORY/resources/fix.n5'\n",
    "mov_path = '/PATH/TO/BIGSTREAM/REPOSITORY/resources/mov.n5'\n",
    "\n",
    "# create Zarr file objects\n",
    "fix_zarr = zarr.open(store=zarr.N5Store(fix_path), mode='r')\n",
    "mov_zarr = zarr.open(store=zarr.N5Store(mov_path), mode='r')\n",
    "\n",
    "# get pointers to the low res scale level\n",
    "# still just pointers, no data loaded into memory yet\n",
    "fix_lowres = fix_zarr['/lowres']\n",
    "mov_lowres = mov_zarr['/lowres']\n",
    "\n",
    "# we need the voxel spacings for the low res data sets\n",
    "# we can compute them from the low res data set metadata\n",
    "fix_meta = fix_lowres.attrs.asdict()\n",
    "mov_meta = mov_lowres.attrs.asdict()\n",
    "fix_lowres_spacing = np.array(fix_meta['pixelResolution']) * fix_meta['downsamplingFactors']\n",
    "mov_lowres_spacing = np.array(mov_meta['pixelResolution']) * mov_meta['downsamplingFactors']\n",
    "fix_lowres_spacing = fix_lowres_spacing[::-1]  # put in zyx order to be consistent with image data\n",
    "mov_lowres_spacing = mov_lowres_spacing[::-1]\n",
    "\n",
    "# read small image data into memory as numpy arrays\n",
    "fix_lowres_data = fix_lowres[...]\n",
    "mov_lowres_data = mov_lowres[...]\n",
    "\n",
    "# sanity check: print the voxel spacings and lowres dataset shapes\n",
    "print(fix_lowres_spacing, mov_lowres_spacing)\n",
    "print(fix_lowres_data.shape, mov_lowres_data.shape)\n",
    "\n",
    "# get pointers to the high res scale level\n",
    "fix_highres = fix_zarr['/highres']\n",
    "mov_highres = mov_zarr['/highres']\n",
    "\n",
    "# we need the voxel spacings for the high res data sets\n",
    "# we can compute them from the high res data set metadata\n",
    "fix_meta = fix_highres.attrs.asdict()\n",
    "mov_meta = mov_highres.attrs.asdict()\n",
    "fix_highres_spacing = np.array(fix_meta['pixelResolution']) * fix_meta['downsamplingFactors']\n",
    "mov_highres_spacing = np.array(mov_meta['pixelResolution']) * mov_meta['downsamplingFactors']\n",
    "fix_highres_spacing = fix_highres_spacing[::-1]\n",
    "mov_highres_spacing = mov_highres_spacing[::-1]\n",
    "\n",
    "# sanity check: print the voxel spacings and lowres dataset shapes\n",
    "print(fix_highres_spacing, mov_highres_spacing)\n",
    "print(fix_highres.shape, mov_highres.shape)\n",
    "\n",
    "# write data to view in fiji or similar\n",
    "# here we write the highres data to disk, which requires loading it all into memory\n",
    "# this is fine for tutorial data, but in practice you would not do this with your\n",
    "# large data\n",
    "tifffile.imsave('./fix_lowres_data.tiff', fix_lowres_data)\n",
    "tifffile.imsave('./mov_lowres_data.tiff', mov_lowres_data)\n",
    "tifffile.imsave('./fix_highres_data.tiff', fix_highres[...])\n",
    "tifffile.imsave('./mov_highres_data.tiff', mov_highres[...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In memory affine registration of smaller images\n",
    "---\n",
    "\n",
    "Here, we will use the `alignment_pipeline` function from the `bigstream.align` module. This function allows you to construct complex alignment pipelines including various affine and deformable stages. We will construct a pipeline to do two steps: (1) a feature point and ransac based affine alignment that will find a good initial fit and (2) a gradient descent based affine alignment that will refine the first step to an even better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should first read the docstrings for alignment_pipeline and apply_transform\n",
    "# to understand more about what we're going to do\n",
    "from bigstream.align import alignment_pipeline\n",
    "from bigstream.transform import apply_transform\n",
    "\n",
    "print(\"alignment_pipeline\\n\", alignment_pipeline.__doc__)\n",
    "print('\\n\\n------------------------')\n",
    "print(\"apply_transform\\n\", apply_transform.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-memory alignment functions are in bigstream.align\n",
    "# functions for resampling data after a transform are found are in bigstream.transform\n",
    "from bigstream.align import alignment_pipeline\n",
    "from bigstream.transform import apply_transform\n",
    "\n",
    "# define arguments for the feature point and ransac stage (you'll understand these later)\n",
    "ransac_kwargs = {'blob_sizes':[6, 20]}\n",
    "\n",
    "# define arguments for the gradient descent stage (you'll understand these later)\n",
    "affine_kwargs = {\n",
    "    'shrink_factors':(2,),\n",
    "    'smooth_sigmas':(2.5,),\n",
    "    'optimizer_args':{\n",
    "        'learningRate':0.25,\n",
    "        'minStep':0.,\n",
    "        'numberOfIterations':400,\n",
    "    },\n",
    "}\n",
    "\n",
    "# define the alignment steps\n",
    "steps = [('ransac', ransac_kwargs), ('affine', affine_kwargs)]\n",
    "\n",
    "# execute the alignment\n",
    "affine = alignment_pipeline(\n",
    "    fix_lowres_data, mov_lowres_data,\n",
    "    fix_lowres_spacing, mov_lowres_spacing,\n",
    "    steps,\n",
    ")\n",
    "\n",
    "# resample the moving image data using the transform you found\n",
    "aligned = apply_transform(\n",
    "    fix_lowres_data, mov_lowres_data,\n",
    "    fix_lowres_spacing, mov_lowres_spacing,\n",
    "    transform_list=[affine,],\n",
    ")\n",
    "\n",
    "# write results\n",
    "np.savetxt('./affine.mat', affine)\n",
    "tifffile.imsave('./affine_lowres.tiff', aligned)\n",
    "\n",
    "# load precomputed result (handy to use later if you've already run the cell)\n",
    "affine = np.loadtxt('./affine.mat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you should pop open an image viewer and look at `fix_lowres_data.tif` and `affine_lowres.tif` to see what the alignment accomplished. Ideally, you should superimpose the aligned data onto the fixed data (e.g. in fiji make a multichannel image) to view the alignment accuracy through the whole volume.\n",
    "\n",
    "Because this tutorial data is pretty small, even these global steps are enough to align the data almost perfectly. In a bit we'll proceed with an unnecessary but instructive blockwise deformable alignment just to teach how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More reading - Get to know the API for fundamental alignment steps\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are all the things you can chain together with alignment_pipeline\n",
    "# you should read the docstrings for these functions to learn more about how to build alignments\n",
    "from bigstream.align import (\n",
    "    feature_point_ransac_affine_align,\n",
    "    random_affine_search,\n",
    "    affine_align,\n",
    "    deformable_align\n",
    ")\n",
    "from bigstream.configure_irm import configure_irm\n",
    "\n",
    "# these are the alignment functions\n",
    "print(\"feature_point_ransac_affine\\n\", feature_point_ransac_affine_align.__doc__)\n",
    "print('\\n\\n------------------------')\n",
    "print(\"random_affine_search\\n\", random_affine_search.__doc__)\n",
    "print('\\n\\n------------------------')\n",
    "print(\"affine_align\\n\", affine_align.__doc__)\n",
    "print('\\n\\n------------------------')\n",
    "print(\"deformable_align\\n\", deformable_align.__doc__)\n",
    "print('\\n\\n------------------------')\n",
    "\n",
    "# also - affine_align and deformable_align both pass additional keyword arguments to this configure_irm function\n",
    "# configure_irm really is the heart of bigstream and to become an expert user you should eventually become familiar with\n",
    "# the SimpleITK ImageRegistrationMethod API. But you can save all that for later.\n",
    "print(\"configure_irm\", configure_irm.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of memory affine and deformable registration of big images\n",
    "---\n",
    "\n",
    "Here we will use the `distributed_piecewise_alignment_pipeline` function from `bigstream.piecewise_align`. This function breaks the images up into blocks and runs the same `alignment_pipeline` function we used before on each of those blocks separately. The results are smoothly stitched back together into a single transform of the entire (large) moving image. In this case, we will again construct an alignment of two steps: (1) a feature point and ransac alignment to find a good initial fit and (2) a deformable alignment that should tighten up even the smallest edges into correspondence.\n",
    "\n",
    "Each block is treated as a separate job and jobs are run as parallel as possible up to the limit of the number of compute workers you have. If you are on a workstation, compute workers will be cpu cores. If you are on a cluster you have more power in determining what a compute worker will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out-of-memory alignment functions are in bigstream.piecewise_align\n",
    "# functions for resampling large images with large transforms are in bigstream.piecewise_transform\n",
    "from bigstream.piecewise_align import distributed_piecewise_alignment_pipeline\n",
    "from bigstream.piecewise_transform import distributed_apply_transform\n",
    "\n",
    "# define arguments for the feature point and ransac stage (if you read the feature_point_ransac_affine_align\n",
    "# docstring you shoud understand your options here now)\n",
    "ransac_kwargs = {'blob_sizes':[6, 20], 'nspots':1000}\n",
    "\n",
    "# define arguments for the deformable stage (if you read the deformable_align docstring and the configure_irm\n",
    "# docstring you should understand your options here now)\n",
    "deform_kwargs = {\n",
    "    'smooth_sigmas':(0.25,),\n",
    "    'control_point_spacing':128.0,\n",
    "    'control_point_levels':(1,),\n",
    "    'optimizer_args':{\n",
    "        'learningRate':0.25,\n",
    "        'minStep':0.,\n",
    "        'numberOfIterations':10,\n",
    "    },\n",
    "}\n",
    "\n",
    "# define the alignment steps\n",
    "steps = [('ransac', ransac_kwargs), ('deform', deform_kwargs)]\n",
    "\n",
    "# define the blocksize for breaking up the image domain, this is in voxels\n",
    "blocksize = [128,]*3\n",
    "\n",
    "# configure options for your compute environment, cluster or local workstation\n",
    "## Distributed computing in bigstream is handled by dask and helped along by another package I wrote\n",
    "## called ClusterWrap. ClusterWrap will use a local_cluster object by default, which will use the\n",
    "## resources of your local machine for distributed computing. If you want to distribute your job\n",
    "## on a cluster, you will need to learn about ClusterWrap and probably dask-jobqueue as well.\n",
    "cluster_kwargs={}\n",
    "\n",
    "# Execute alignment, note we are providing the global alignment found with the lowres data as\n",
    "# an initialization. Also note, the deform will be stored as a zarr array on disk.\n",
    "deform = distributed_piecewise_alignment_pipeline(\n",
    "    fix_highres, mov_highres,\n",
    "    fix_highres_spacing, mov_highres_spacing,\n",
    "    steps,\n",
    "    blocksize=blocksize,\n",
    "    write_path='./deform.zarr',\n",
    "    static_transform_list=[affine,],\n",
    "    cluster_kwargs=cluster_kwargs,\n",
    ")\n",
    "\n",
    "# apply the two transforms we found (global affine from lowres data and local deform from highres data)\n",
    "# the resampled data is now located in a zarr array on disk\n",
    "aligned = distributed_apply_transform(\n",
    "    fix_highres, mov_highres,\n",
    "    fix_highres_spacing, mov_highres_spacing,\n",
    "    transform_list=[affine, deform],\n",
    "    blocksize=blocksize,\n",
    "    write_path='./deformed.zarr',\n",
    "    cluster_kwargs=cluster_kwargs,\n",
    ")\n",
    "\n",
    "# We'll write the deformed data into a format that most viewers can read\n",
    "# you wouldn't normally be able to do this for huge images, you would either look at smaller crops\n",
    "# or use something like bigdataviewer\n",
    "tifffile.imsave('./deformed.tiff', aligned[...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you should pop open an image viewer and look at `fix_highres_data.tiff` and `deformed.tiff` to see what the alignment accomplished. Ideally, you should superimpose the aligned data onto the fixed data (e.g. in fiji make a multichannel image) to view the alignment accuracy through the whole volume.\n",
    "\n",
    "For a real out-of-memory application, images could be 100x larger than the example images here. In such a case you will see a significant different between the quality of the global low resolution alignment and the local out-of-memory high resolution alignment. The strategy shown in this tutorial is still the right one however, you should align your data in stages from coarse to fine - global alignments at lower resolution followed by local alignments at higher resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "---\n",
    "\n",
    "Where to go from here? I suggest trying to adapt a copy of this notebook to data of your own. Get comfortable with the image formats used here (most importantly zarr). Get more knowledgeable about the `bigstream.align`, `bigstream.transform`, `bigstream.piecewise_align`, and `bigstream.piecewise_transform` APIs. Look into the `bigstream.configure_irm` API and the `SimpleITK.ImageRegistrationMethod` API. These are the essential ingredients for doing big data registrations.\n",
    "\n",
    "Another important step is getting to know your compute environment. If you are computing on a local workstation, you should know how to parameterize dask to create workers on your machine. Similarly, if you're on a cluster you should know how to parameterize dask to create workers in that environment. All of these things can be specified to bigstream through the `cluster_kwargs` argument shown in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigstream",
   "language": "python",
   "name": "bigstream"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
