{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alignment with bigstream\n",
    "---\n",
    "\n",
    "BigStream is a library of tools for image registration of huge images. It uses big data tools like Zarr and DASK to enable working with too-large-for-memory datasets and to make costly alignments finish in practical amounts of time by distributing the work on your workstation or cluster.\n",
    "\n",
    "This tutorial will walk you through the following steps:\n",
    "\n",
    "    1. Reading image data and metadata using Zarr\n",
    "    2. Global affine alignment\n",
    "    3. Local affine alignments\n",
    "    4. Local deformable alignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary\n",
    "---\n",
    "\n",
    "Make sure BigStream is installed: `pip install bigstream`\n",
    "\n",
    "You should also get the source code, which is located here: https://github.com/GFleishman/bigstream. \\\n",
    "Follow the instrucions on github to clone the repository, which contains the example data used for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial data\n",
    "---\n",
    "\n",
    "In the BigStream repository the `resources` folder contains two images in N5 format.\\\n",
    "We will first access the data and metadata in these files using Zarr, which was installed with BigStream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library for reading\n",
    "import zarr\n",
    "\n",
    "# file paths to tutorial data N5 files\n",
    "fix_path = '/groups/scicompsoft/home/fleishmang/source/bigstream/resources/fix.n5'\n",
    "mov_path = '/groups/scicompsoft/home/fleishmang/source/bigstream/resources/mov.n5'\n",
    "\n",
    "# create Zarr file object using N5Stores\n",
    "fix_zarr = zarr.open(store=zarr.N5Store(fix_path), mode='r')\n",
    "mov_zarr = zarr.open(store=zarr.N5Store(mov_path), mode='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fix_zarr` and `mov_zarr` are just lazy pointers to the N5 files; no image data has been loaded into memory yet.\\\n",
    "The first alignment step, global affine, only needs low resolution data; which we assume fits into available memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll need numpy now\n",
    "import numpy as np\n",
    "\n",
    "# get pointers to the low res scale level\n",
    "# still just pointers, no data loaded into memory yet\n",
    "fix_lowres = fix_zarr['/lowres']\n",
    "mov_lowres = mov_zarr['/lowres']\n",
    "\n",
    "# we need the voxel spacings for the low res data sets\n",
    "# we can compute them from the low res data set metadata\n",
    "fix_meta = fix_lowres.attrs.asdict()\n",
    "mov_meta = mov_lowres.attrs.asdict()\n",
    "fix_lowres_spacing = np.array(fix_meta['pixelResolution']) * fix_meta['downsamplingFactors']\n",
    "mov_lowres_spacing = np.array(mov_meta['pixelResolution']) * mov_meta['downsamplingFactors']\n",
    "\n",
    "# read data into memory as numpy arrays\n",
    "# Why transpose? zarr reads data as zyx, but we prefer xyz (metadata is already xyz)\n",
    "fix_lowres_data = fix_lowres[...].transpose(2, 1, 0)\n",
    "mov_lowres_data = mov_lowres[...].transpose(2, 1, 0)\n",
    "\n",
    "# sanity check: print the voxel spacings and lowres dataset shapes\n",
    "print(fix_lowres_spacing, mov_lowres_spacing)\n",
    "print(fix_lowres_data.shape, mov_lowres_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global affine alignment\n",
    "---\n",
    "\n",
    "The affine alignment algorithm is composed of these steps:\n",
    "\n",
    "    1. Key point extraction from fixed and moving images\n",
    "    2. Correspondence matching between key points in the fixed and moving point sets using neighborhood correlation\n",
    "    3. Affine alignment using a RANSAC filter on the point correspondences\n",
    "\n",
    "But this is all accomplished with one function call. The following cell will take some time to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affine alignment functions are in bigstream.affine\n",
    "from bigstream import affine\n",
    "\n",
    "# see below for explanation of parameters\n",
    "global_affine = affine.ransac_affine(\n",
    "    fix_lowres_data, mov_lowres_data,\n",
    "    fix_lowres_spacing, mov_lowres_spacing,\n",
    "    min_radius=6, max_radius=20, match_threshold=0.75,\n",
    ")\n",
    "\n",
    "# sanity check: print the result\n",
    "print(global_affine)\n",
    "\n",
    "# For tutorial data, should be approximately:\n",
    "# [[ 9.91070543e-01  3.40205967e-02 -5.63125159e-03 -8.12590407e+01]\n",
    "#  [-3.86715664e-02  1.02023436e+00 -6.30453307e-03 -1.33468687e+01]\n",
    "#  [ 1.19321249e-02 -2.21371673e-02  9.67441910e-01 -2.42373194e-01]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and output\n",
    "\n",
    "    min_radius: radius in voxels of smallest expected blob/cell size\n",
    "    \n",
    "    max_radius: radius in voxels of largest expected blob/cell size\n",
    "    \n",
    "    match_threshold: neighborhood correlation between two key points must exceed this value for it to be a valid match\n",
    "    \n",
    "Other optional parameters are:\n",
    "    \n",
    "    cc_radius: key points are matched using correlation of the data in their neighborhoods, this is the neighborhood radius in voxels (default: 12)\n",
    "    \n",
    "    nspots: the maximum number of key point pairs to use to compute the affine alignment (default: 5000)\n",
    "    \n",
    "    align_threshold: points are considered aligned by the affine if they are less than this value apart, in micrometers (default: 2.0)\n",
    "    \n",
    "    num_sigma_max: the maximum number of filters to run in the blob detection (default: 10)\n",
    "    \n",
    "The return value is:\n",
    "    \n",
    "    global_affine: the return value is a 3x4 affine transform matrix as a numpy array; this describes correspondence between points in the fixed image and moving image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying global affine\n",
    "\n",
    "The alignment only gave us the affine transform matrix. Here we apply it to the moving image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for applying transforms are in bigstream.transform\n",
    "from bigstream import transform\n",
    "\n",
    "# apply the global affine to the moving image\n",
    "mov_lowres_aligned = transform.apply_global_affine(\n",
    "    fix_lowres_data, mov_lowres_data,\n",
    "    fix_lowres_spacing, mov_lowres_spacing,\n",
    "    global_affine,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the fixed, moving, and aligned data. Try running this cell a few times with different values for `slc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll visualize the results with some image plots\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot some image slices to check on things\n",
    "slc = 70\n",
    "f_slc = fix_lowres_data[..., slc]\n",
    "a_slc = mov_lowres_aligned[..., slc]\n",
    "m_slc = mov_lowres_data[..., slc]\n",
    "\n",
    "# normalize for display\n",
    "f_slc = f_slc.astype(np.float32) / f_slc.max()\n",
    "a_slc = a_slc.astype(np.float32) / a_slc.max()\n",
    "m_slc = m_slc.astype(np.float32) / m_slc.max()\n",
    "\n",
    "# make RGB versions\n",
    "f_rgb = np.zeros(f_slc.shape + (3,))\n",
    "f_rgb[..., 0] = f_slc * 2\n",
    "a_rgb = np.zeros(a_slc.shape + (3,))\n",
    "a_rgb[..., 0] = f_slc * 2\n",
    "a_rgb[..., 1] = a_slc * 2\n",
    "m_rgb = np.zeros(m_slc.shape + (3,))\n",
    "m_rgb[..., 1] = m_slc * 2\n",
    "\n",
    "# create figure and subplots\n",
    "fig = plt.figure(figsize=(12,24))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(f_rgb)\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(f_rgb)\n",
    "plt.imshow(a_rgb)\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(m_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local affine alignments\n",
    "---\n",
    "\n",
    "The tutorial dataset is small, so the global affine lines up almost all of the data well. But with a real dataset we would likely need to refine the global alignment with local alignments. In local affine alignment, the images are carved into overlapping tiles and a separate affine is computed for each tile. For large data sets there can be many tiles. To make this process tractable a cluster is constructed using [ClusterWrap](https://github.com/GFleishman/ClusterWrap) and DASK. The local affines are all computed in parallel on their own workers.\n",
    "\n",
    "\n",
    "The `tiled_ransac_affine` has a few new arguments related to the tiling:\n",
    "\n",
    "    blocksize: iterable, length equal to the image dimension. The size of tiles in voxels.\n",
    "    \n",
    "    cluster_kwargs: extra arguments to the cluster constructor. See ClusterWrap for details.\n",
    "\n",
    "All the optional arguments from `ransac_affine` are available here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note use of mov_lowres_aligned as moving image rather than mov_lowres_data\n",
    "# Note also that fix_lowres_spacing is used as the \"moving\" voxel spacing here\n",
    "local_affines = affine.tiled_ransac_affine(\n",
    "    fix_lowres_data, mov_lowres_aligned,\n",
    "    fix_lowres_spacing, fix_lowres_spacing,\n",
    "    min_radius=6, max_radius=20, match_threshold=0.75,\n",
    "    blocksize=[128,]*3,\n",
    ")\n",
    "\n",
    "# sanity check, print shape of local_affines\n",
    "# the first three dimensions index over the tiles\n",
    "# the last two dimensions are the 3x4 affine matrices for each tile\n",
    "print(local_affines.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying local affines\n",
    "\n",
    "This function is conceptually similar to `apply_global_affine` used previously, but similar to `tiled_ransac_affine` this function distributes the computation over tiles. You need to provide *the same* blocksize used for `tiled_ransac_affine`. Note also that you should include the `global_affine` to ensure both transforms are applied.\n",
    "\n",
    "There are 3 options for the output of this function. Which of the three options you get is controlled through 2 new parameters:\n",
    "\n",
    "    write_path: Default is None. But if a file path is provided, the aligned data will be written as a zarr file to disk and a reference to that zarr.Array object is returned. This is useful when your data is too large to fit into memory.\n",
    "    \n",
    "    lazy: True or False. If True, the resampling computation is not performed, only a reference to the lazy dask.Array object is returned. If False, the computation is executed but rather than writing to disk (the case if a write_path is given), the result is returned. You must be sure your data fits into RAM to choose this option.\n",
    "\n",
    "Since we're working with the lowres data - we omit `write_path` and set `lazy=False`, but if we were applying local affines to high resolution data, or a very large dataset in general, we would use `write_path=/path/to/output.zarr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the local affines to the moving image\n",
    "#   Note we're using mov_lowres_data again - it's better\n",
    "#   to provide the global and local affines together. They\n",
    "#   are composed into a single transform - that way the moving\n",
    "#   image is only resampled one time.\n",
    "mov_lowres_aligned = transform.apply_local_affines(\n",
    "    fix_lowres_data, mov_lowres_data,\n",
    "    fix_lowres_spacing, mov_lowres_spacing,\n",
    "    local_affines,\n",
    "    blocksize=[128,]*3,\n",
    "    global_affine=global_affine,\n",
    "    lazy=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to before, we can inspect the alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some image slices to check on things\n",
    "slc = 70\n",
    "f_slc = fix_lowres_data[..., slc]\n",
    "a_slc = mov_lowres_aligned[..., slc]\n",
    "m_slc = mov_lowres_data[..., slc]\n",
    "\n",
    "# normalize for display\n",
    "f_slc = f_slc.astype(np.float32) / f_slc.max()\n",
    "a_slc = a_slc.astype(np.float32) / a_slc.max()\n",
    "m_slc = m_slc.astype(np.float32) / m_slc.max()\n",
    "\n",
    "# make RGB versions\n",
    "f_rgb = np.zeros(f_slc.shape + (3,))\n",
    "f_rgb[..., 0] = f_slc * 2\n",
    "a_rgb = np.zeros(a_slc.shape + (3,))\n",
    "a_rgb[..., 0] = f_slc * 2\n",
    "a_rgb[..., 1] = a_slc * 2\n",
    "m_rgb = np.zeros(m_slc.shape + (3,))\n",
    "m_rgb[..., 1] = m_slc * 2\n",
    "\n",
    "# create figure and subplots\n",
    "fig = plt.figure(figsize=(12,24))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(f_rgb)\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(f_rgb)\n",
    "plt.imshow(a_rgb)\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(m_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local deformable alignments\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deformable alignment is the final step, where every voxel is assigned its own displacement vector. Deformable alignment can take advantage of high resolution features, so we'll be using larger arrays of higher resolution data. Here we assume the data is too large for memory on a single machine and the computation would be intractably slow using a single machine. This section shows how BigStream's use of distributed computing enables you to align your data despite of these difficulties.\n",
    "\n",
    "Compare the cell below to the second cell in the _Tutorial Data_ section. Note that here we do not read the data into memory but proceed with the lazy zarr.Array objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pointers to the high res scale level\n",
    "# still just pointers, no data loaded into memory\n",
    "fix_highres = fix_zarr['/highres']\n",
    "mov_highres = mov_zarr['/highres']\n",
    "\n",
    "# we need the voxel spacings for the high res data sets\n",
    "# we can compute them from the high res data set metadata\n",
    "fix_meta = fix_highres.attrs.asdict()\n",
    "mov_meta = mov_highres.attrs.asdict()\n",
    "fix_highres_spacing = np.array(fix_meta['pixelResolution']) * fix_meta['downsamplingFactors']\n",
    "mov_highres_spacing = np.array(mov_meta['pixelResolution']) * mov_meta['downsamplingFactors']\n",
    "\n",
    "# sanity check: print the voxel spacings and lowres dataset shapes\n",
    "print(fix_highres_spacing, mov_highres_spacing)\n",
    "print(fix_highres.shape, mov_highres.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the voxel spacings are in xyz order but the lazy array shapes are still zyx.\n",
    "\n",
    "\n",
    "Before we can deform, we need a highres version of the moving data with the global and local affines applied. Compare the cell below to the first cell in the _Apply local affines_ subsection. First note we provide the highres lazy zarr.Arrays as inputs. We happen to know they are 2x larger along each axis - so importantly the `blocksize` parameter has been doubled. We are also assuming the aligned data will not fit into memory, so here we use the `write_path` parameter discussed previously. Finally, there is a new parameter:\n",
    "\n",
    "    transpose: List of 3 boolean values. Should we transpose the axis order of the fixed image, moving image, or transform data.\n",
    "\n",
    "In this case, since the fixed and moving image data are being read from zarr files, they must be transposed - but because the transform is being constructed from the global and local affine matrices, it does not need to be transposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this time, we'll write the aligned data to disk\n",
    "write_path = './mov_highres_affine_aligned.zarr'\n",
    "\n",
    "# apply the affines to the highres moving image\n",
    "mov_highres_aligned = transform.apply_local_affines(\n",
    "    fix_highres, mov_highres,\n",
    "    fix_highres_spacing, mov_highres_spacing,\n",
    "    local_affines,\n",
    "    blocksize=[256,]*3,\n",
    "    global_affine=global_affine,\n",
    "    write_path=write_path,\n",
    "    transpose=[True, True, False],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the application was successful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some image slices to check on things\n",
    "#   Note for highres data we doubled the slice number\n",
    "#   The fix/mov images still need to be transposed,\n",
    "#   But mov_highres_aligned, which we made in the previous step\n",
    "#   was written out in xyz order\n",
    "slc = 140\n",
    "f_slc = fix_highres[slc, ...].transpose(1,0)\n",
    "a_slc = mov_highres_aligned[..., slc]\n",
    "m_slc = mov_highres[slc, ...].transpose(1,0)\n",
    "\n",
    "# normalize for display\n",
    "f_slc = f_slc.astype(np.float32) / f_slc.max()\n",
    "a_slc = a_slc.astype(np.float32) / a_slc.max()\n",
    "m_slc = m_slc.astype(np.float32) / m_slc.max()\n",
    "\n",
    "# make RGB versions\n",
    "f_rgb = np.zeros(f_slc.shape + (3,))\n",
    "f_rgb[..., 0] = f_slc * 2\n",
    "a_rgb = np.zeros(a_slc.shape + (3,))\n",
    "a_rgb[..., 0] = f_slc * 2\n",
    "a_rgb[..., 1] = a_slc * 2\n",
    "m_rgb = np.zeros(m_slc.shape + (3,))\n",
    "m_rgb[..., 1] = m_slc * 2\n",
    "\n",
    "# create figure and subplots\n",
    "fig = plt.figure(figsize=(12,24))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(f_rgb)\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(f_rgb)\n",
    "plt.imshow(a_rgb)\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(m_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can execute the deformable alignment on the highres data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deform functions are in bistream.deform\n",
    "from bigstream import deform\n",
    "\n",
    "# a write location for the final transform\n",
    "write_path = './full_highres_transform.zarr'\n",
    "\n",
    "full_field = deform.tiled_deformable_align(\n",
    "    fix_highres, mov_highres_aligned,\n",
    "    fix_highres_spacing, fix_highres_spacing,\n",
    "    blocksize=[256,]*3,\n",
    "    transpose=[True, False],\n",
    "    global_affine=global_affine, local_affines=local_affines,\n",
    "    write_path=write_path,\n",
    "    deform_kwargs={'iterations':[1, 1]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying full transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a write path for the final result\n",
    "write_path = './mov_highres_deform_aligned.zarr'\n",
    "\n",
    "# apply the local affines to the moving image\n",
    "mov_highres_aligned = transform.apply_position_field(\n",
    "    fix_highres, mov_highres,\n",
    "    fix_highres_spacing, mov_highres_spacing,\n",
    "    full_field,\n",
    "    blocksize=[256,]*3,\n",
    "    transpose=[True, True, False],\n",
    "    write_path=write_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some image slices to check on things\n",
    "slc = 140\n",
    "f_slc = fix_highres[slc, ...].transpose(1,0)\n",
    "a_slc = mov_highres_aligned[..., slc]\n",
    "m_slc = mov_highres[slc, ...].transpose(1,0)\n",
    "\n",
    "# normalize for display\n",
    "f_slc = f_slc.astype(np.float32) / f_slc.max()\n",
    "a_slc = a_slc.astype(np.float32) / a_slc.max()\n",
    "m_slc = m_slc.astype(np.float32) / m_slc.max()\n",
    "\n",
    "# make RGB versions\n",
    "f_rgb = np.zeros(f_slc.shape + (3,))\n",
    "f_rgb[..., 0] = f_slc * 2\n",
    "a_rgb = np.zeros(a_slc.shape + (3,))\n",
    "a_rgb[..., 0] = f_slc * 2\n",
    "a_rgb[..., 1] = a_slc * 2\n",
    "m_rgb = np.zeros(m_slc.shape + (3,))\n",
    "m_rgb[..., 1] = m_slc * 2\n",
    "\n",
    "# create figure and subplots\n",
    "fig = plt.figure(figsize=(12,24))\n",
    "fig.add_subplot(1, 3, 1)\n",
    "plt.imshow(f_rgb)\n",
    "fig.add_subplot(1, 3, 2)\n",
    "plt.imshow(f_rgb)\n",
    "plt.imshow(a_rgb)\n",
    "fig.add_subplot(1, 3, 3)\n",
    "plt.imshow(m_rgb)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
