{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RootLogger root (DEBUG)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bigstream.configure_logging import configure_logging\n",
    "\n",
    "configure_logging(None, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inputs\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'zlib'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'gzip'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'bz2'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'lzma'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'blosc'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'zstd'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'lz4'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'astype'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'delta'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'quantize'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'fixedscaleoffset'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'packbits'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'categorize'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'pickle'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'base64'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'shuffle'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'bitround'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'msgpack2'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'crc32'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'adler32'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'jenkins_lookup3'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'json2'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'vlen-utf8'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'vlen-bytes'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'vlen-array'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'fletcher32'\n",
      "2024-06-18 10:26:20 - numcodecs - DEBUG - Registering codec 'n5_wrapper'\n",
      "Fix attrs: {'downsamplingFactors': [1.0, 1.0, 1.0], 'pixelResolution': {'dimensions': [1.0, 1.0, 1.0], 'unit': 'pixel'}}\n",
      "Mov attrs: {'downsamplingFactors': [1.0, 1.0, 1.0], 'pixelResolution': {'dimensions': [1.0, 1.0, 1.0], 'unit': 'pixel'}}\n"
     ]
    }
   ],
   "source": [
    "# reading data and working with arrays\n",
    "import os\n",
    "import zarr, nrrd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# data paths\n",
    "base_path = '/Users/goinac/Work/HHMI/bioimagetools/nextflow-modules'\n",
    "p = f'{base_path}/mylocal/bigstream-testdata3'\n",
    "fix_path = f'{p}/fix.n5'\n",
    "ref_ch = 'c0'\n",
    "fix_s0_subpath = f'{ref_ch}/s0'\n",
    "mov_path = f'{p}/mov.n5'\n",
    "mov_s0_subpath = f'{ref_ch}/s0'\n",
    "exp_factor = 1\n",
    "\n",
    "out_p = f'{base_path}/mylocal/results/bigstream-python'\n",
    "os.makedirs(out_p, exist_ok=True)\n",
    "\n",
    "# load fix data and spacing\n",
    "fix_zarr = zarr.open(store=zarr.N5Store(fix_path), mode='r')\n",
    "fix_meta = fix_zarr[fix_s0_subpath].attrs.asdict()\n",
    "print(f'Fix attrs: {fix_meta}')\n",
    "\n",
    "# load mov data and spacing\n",
    "mov_zarr = zarr.open(store=zarr.N5Store(mov_path), mode='r')\n",
    "mov_meta = mov_zarr[mov_s0_subpath].attrs.asdict()\n",
    "print(f'Mov attrs: {mov_meta}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix_spacing_s0: [1. 1. 1.]\n",
      "fix_spacing_s1: [2. 2. 2.]\n",
      "fix_spacing_s2: [4. 4. 4.]\n",
      "fix_spacing_s3: [8. 8. 8.]\n",
      "fix_spacing_s4: [16. 16. 16.]\n",
      "fix_spacing_s5: [32. 32. 32.]\n",
      "mov_spacing_s0: [1. 1. 1.]\n",
      "mov_spacing_s1: [2. 2. 2.]\n",
      "mov_spacing_s2: [4. 4. 4.]\n",
      "mov_spacing_s3: [8. 8. 8.]\n",
      "mov_spacing_s4: [16. 16. 16.]\n",
      "mov_spacing_s5: [32. 32. 32.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fix_spacing_s0 = np.array(fix_meta['pixelResolution']['dimensions'][::-1]) / exp_factor\n",
    "fix_spacing_s1 = fix_spacing_s0 * [2, 2, 2]\n",
    "fix_spacing_s2 = fix_spacing_s0 * [4, 4, 4]\n",
    "fix_spacing_s3 = fix_spacing_s0 * [8, 8, 8]\n",
    "fix_spacing_s4 = fix_spacing_s0 * [16, 16, 16]\n",
    "fix_spacing_s5 = fix_spacing_s0 * [32, 32, 32]\n",
    "print(f'fix_spacing_s0: {fix_spacing_s0}')\n",
    "print(f'fix_spacing_s1: {fix_spacing_s1}')\n",
    "print(f'fix_spacing_s2: {fix_spacing_s2}')\n",
    "print(f'fix_spacing_s3: {fix_spacing_s3}')\n",
    "print(f'fix_spacing_s4: {fix_spacing_s4}')\n",
    "print(f'fix_spacing_s5: {fix_spacing_s5}')\n",
    "\n",
    "mov_spacing_s0 = np.array(mov_meta['pixelResolution']['dimensions'][::-1]) / exp_factor\n",
    "mov_spacing_s1 = mov_spacing_s0 * [2, 2, 2]\n",
    "mov_spacing_s2 = mov_spacing_s0 * [4, 4, 4]\n",
    "mov_spacing_s3 = mov_spacing_s0 * [8, 8, 8]\n",
    "mov_spacing_s4 = mov_spacing_s0 * [16, 16, 16]\n",
    "mov_spacing_s5 = mov_spacing_s0 * [32, 32, 32]\n",
    "\n",
    "print(f'mov_spacing_s0: {mov_spacing_s0}')\n",
    "print(f'mov_spacing_s1: {mov_spacing_s1}')\n",
    "print(f'mov_spacing_s2: {mov_spacing_s2}')\n",
    "print(f'mov_spacing_s3: {mov_spacing_s3}')\n",
    "print(f'mov_spacing_s4: {mov_spacing_s4}')\n",
    "print(f'mov_spacing_s5: {mov_spacing_s5}')\n",
    "\n",
    "global_scale = 's0'\n",
    "fix_affine_spacing = fix_spacing_s0\n",
    "mov_affine_spacing = mov_spacing_s0\n",
    "\n",
    "local_scale = 's0'\n",
    "fix_deform_spacing = fix_spacing_s0\n",
    "mov_deform_spacing = mov_spacing_s0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Alignment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.84386100e-01, -4.44111772e-04,  1.01842376e-04,\n",
       "         3.04607048e-02],\n",
       "       [-1.39465551e-02,  9.96547376e-01, -6.27954979e-03,\n",
       "         2.54243432e+01],\n",
       "       [-5.16714859e-03, -1.13662287e-02,  9.97928648e-01,\n",
       "        -4.42017886e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load precomputed results\n",
    "affine = np.loadtxt(out_p+'/affine.mat')\n",
    "affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare cluster args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_kwargs={\n",
    "    'config':{\n",
    "        'distributed.worker.memory.target':0.9,\n",
    "        'distributed.worker.memory.spill':0.9,\n",
    "        'distributed.worker.memory.pause':0.9,\n",
    "        'distributed.comm.retry.count':10,\n",
    "        'distributed.comm.timeouts.connect':'600s',\n",
    "        'distributed.scheduler.worker-saturation': 1,\n",
    "        'distributed.scheduler.unknown-task-duration':'60m',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate deform transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:26:22 - asyncio - DEBUG - Using selector: KqueueSelector\n",
      "2024-06-18 10:26:22 - distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "2024-06-18 10:26:22 - distributed.scheduler - INFO - State start\n",
      "2024-06-18 10:26:22 - distributed.scheduler - INFO -   Scheduler at: tcp://192.168.1.219:64309\n",
      "2024-06-18 10:26:22 - distributed.scheduler - INFO -   dashboard at:  http://192.168.1.219:8787/status\n",
      "2024-06-18 10:26:22 - distributed.scheduler - INFO - Registering Worker plugin shuffle\n",
      "2024-06-18 10:26:22 - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.1.219:64312'\n",
      "2024-06-18 10:26:22 - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.1.219:64314'\n",
      "2024-06-18 10:26:22 - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.1.219:64316'\n",
      "2024-06-18 10:26:22 - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.1.219:64318'\n",
      "2024-06-18 10:26:22 - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.1.219:64320'\n",
      "2024-06-18 10:26:23 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.1.219:64324', name: 0, status: init, memory: 0, processing: 0>\n",
      "2024-06-18 10:26:23 - distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.1.219:64324\n",
      "2024-06-18 10:26:23 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64326\n",
      "2024-06-18 10:26:23 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.1.219:64327', name: 1, status: init, memory: 0, processing: 0>\n",
      "2024-06-18 10:26:23 - distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.1.219:64327\n",
      "2024-06-18 10:26:23 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64329\n",
      "2024-06-18 10:26:23 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.1.219:64330', name: 2, status: init, memory: 0, processing: 0>\n",
      "2024-06-18 10:26:23 - distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.1.219:64330\n",
      "2024-06-18 10:26:23 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64334\n",
      "2024-06-18 10:26:23 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.1.219:64331', name: 3, status: init, memory: 0, processing: 0>\n",
      "2024-06-18 10:26:23 - distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.1.219:64331\n",
      "2024-06-18 10:26:23 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64336\n",
      "2024-06-18 10:26:23 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.1.219:64332', name: 4, status: init, memory: 0, processing: 0>\n",
      "2024-06-18 10:26:23 - distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.1.219:64332\n",
      "2024-06-18 10:26:23 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64338\n",
      "2024-06-18 10:26:23 - distributed.scheduler - INFO - Receive client connection: Client-bd30944a-2d7e-11ef-a79b-fe4548c183a7\n",
      "2024-06-18 10:26:23 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64339\n",
      "2024-06-18 10:26:23 - bigstream.piecewise_align - INFO - Partition (29, 266, 226) into [1 3 2] using[128 128 128] and [64 64 64]\n",
      "2024-06-18 10:26:30 - distributed.scheduler - INFO - Receive client connection: Client-worker-c11b46a4-2d7e-11ef-a806-fe4548c183a7\n",
      "2024-06-18 10:26:30 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/bigstream/lib/python3.11/site-packages/bigstream/piecewise_align.py:384: RuntimeWarning: invalid value encountered in divide\n",
      "  weights = weights / (1 - missing_weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:26:30 - distributed.scheduler - INFO - Receive client connection: Client-worker-c1545980-2d7e-11ef-a802-fe4548c183a7\n",
      "2024-06-18 10:26:30 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/bigstream/lib/python3.11/site-packages/bigstream/piecewise_align.py:384: RuntimeWarning: invalid value encountered in divide\n",
      "  weights = weights / (1 - missing_weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:26:36 - distributed.scheduler - INFO - Receive client connection: Client-worker-c503397a-2d7e-11ef-a803-fe4548c183a7\n",
      "2024-06-18 10:26:36 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/bigstream/lib/python3.11/site-packages/bigstream/piecewise_align.py:384: RuntimeWarning: invalid value encountered in divide\n",
      "  weights = weights / (1 - missing_weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:26:37 - distributed.scheduler - INFO - Receive client connection: Client-worker-c559a29c-2d7e-11ef-a805-fe4548c183a7\n",
      "2024-06-18 10:26:37 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/bigstream/lib/python3.11/site-packages/bigstream/piecewise_align.py:384: RuntimeWarning: invalid value encountered in divide\n",
      "  weights = weights / (1 - missing_weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:26:37 - distributed.scheduler - INFO - Receive client connection: Client-worker-c58eee2a-2d7e-11ef-a804-fe4548c183a7\n",
      "2024-06-18 10:26:37 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/mambaforge/base/envs/bigstream/lib/python3.11/site-packages/bigstream/piecewise_align.py:384: RuntimeWarning: invalid value encountered in divide\n",
      "  weights = weights / (1 - missing_weights)\n",
      "/opt/homebrew/Caskroom/mambaforge/base/envs/bigstream/lib/python3.11/site-packages/bigstream/piecewise_align.py:384: RuntimeWarning: invalid value encountered in divide\n",
      "  weights = weights / (1 - missing_weights)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-bd30944a-2d7e-11ef-a79b-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64339; closing.\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-bd30944a-2d7e-11ef-a79b-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Close client connection: Client-bd30944a-2d7e-11ef-a79b-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Retire worker addresses (0, 1, 2, 3, 4)\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.1.219:64312'. Reason: nanny-close\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.1.219:64314'. Reason: nanny-close\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.1.219:64316'. Reason: nanny-close\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.1.219:64318'. Reason: nanny-close\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.1.219:64320'. Reason: nanny-close\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-worker-c1545980-2d7e-11ef-a802-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64348; closing.\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-worker-c503397a-2d7e-11ef-a803-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64350; closing.\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-worker-c58eee2a-2d7e-11ef-a804-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64352; closing.\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-worker-c1545980-2d7e-11ef-a802-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-worker-c503397a-2d7e-11ef-a803-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-worker-c58eee2a-2d7e-11ef-a804-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-worker-c559a29c-2d7e-11ef-a805-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64351; closing.\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-worker-c11b46a4-2d7e-11ef-a806-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64347; closing.\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-worker-c559a29c-2d7e-11ef-a805-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove client Client-worker-c11b46a4-2d7e-11ef-a806-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64326; closing.\n",
      "2024-06-18 10:26:38 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64329; closing.\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Close client connection: Client-worker-c1545980-2d7e-11ef-a802-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Close client connection: Client-worker-c503397a-2d7e-11ef-a803-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Close client connection: Client-worker-c58eee2a-2d7e-11ef-a804-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://192.168.1.219:64324', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718720798.25573')\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://192.168.1.219:64327', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718720798.258643')\n",
      "2024-06-18 10:26:38 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64334; closing.\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://192.168.1.219:64330', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718720798.26272')\n",
      "2024-06-18 10:26:38 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64336; closing.\n",
      "2024-06-18 10:26:38 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64338; closing.\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Close client connection: Client-worker-c559a29c-2d7e-11ef-a805-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Close client connection: Client-worker-c11b46a4-2d7e-11ef-a806-fe4548c183a7\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://192.168.1.219:64331', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718720798.268784')\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://192.168.1.219:64332', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718720798.2695239')\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Lost all workers\n",
      "2024-06-18 10:26:38 - distributed.batched - INFO - Batched Comm Closed <TCP (closed) Scheduler connection to worker local=tcp://192.168.1.219:64309 remote=tcp://192.168.1.219:64334>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/bigstream/lib/python3.11/site-packages/distributed/batched.py\", line 115, in _background_send\n",
      "    nbytes = yield coro\n",
      "             ^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/bigstream/lib/python3.11/site-packages/tornado/gen.py\", line 766, in run\n",
      "    value = future.result()\n",
      "            ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/Caskroom/mambaforge/base/envs/bigstream/lib/python3.11/site-packages/distributed/comm/tcp.py\", line 262, in write\n",
      "    raise CommClosedError()\n",
      "distributed.comm.core.CommClosedError\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Scheduler closing due to unknown reason...\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Scheduler closing all comms\n",
      "CPU times: user 1.96 s, sys: 557 ms, total: 2.51 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from bigstream.piecewise_align import distributed_piecewise_alignment_pipeline\n",
    "\n",
    "# FASTER\n",
    "\n",
    "# get full resolution data\n",
    "fix = fix_zarr[f'/{ref_ch}/{local_scale}']\n",
    "mov = mov_zarr[f'/{ref_ch}/{local_scale}']\n",
    "\n",
    "# define alignment steps\n",
    "affine_kwargs = {\n",
    "    'smooth_sigmas':[0.25],\n",
    "    'optimizer_args':{\n",
    "        'learningRate':0.25,\n",
    "        'minStep':0.,\n",
    "        'numberOfIterations':25,\n",
    "    },\n",
    "    \"alignment_spacing\": 1.0,\n",
    "}\n",
    "\n",
    "deform_kwargs = {\n",
    "    'smooth_sigmas':[0.25],\n",
    "    'control_point_spacing':50.0,\n",
    "    'control_point_levels':[1],\n",
    "    'optimizer_args':{\n",
    "        'learningRate':2.5,\n",
    "        'minStep':0.,\n",
    "        'numberOfIterations':25,\n",
    "    },\n",
    "    \"alignment_spacing\": 2.0,\n",
    "}\n",
    "\n",
    "steps = [ ('affine', affine_kwargs,), ('deform', deform_kwargs,), ]\n",
    "\n",
    "# deform\n",
    "deform = distributed_piecewise_alignment_pipeline(\n",
    "    fix, mov,\n",
    "    fix_deform_spacing, mov_deform_spacing,\n",
    "    steps=steps,\n",
    "    blocksize=[128, 128, 128],\n",
    "    static_transform_list=[affine,],\n",
    "    write_path=out_p+'/deform.zarr',\n",
    "    cluster_kwargs=cluster_kwargs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read already saved result\n",
    "deform = zarr.open(out_p+'/deform.zarr', mode='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deform[0:10,0,0] [[0.06829195 1.022542   1.5290565 ]\n",
      " [0.07427759 1.036904   1.5449455 ]\n",
      " [0.08043365 1.0498701  1.5581748 ]\n",
      " [0.08654308 1.0613896  1.568802  ]\n",
      " [0.09261805 1.0714942  1.5769047 ]\n",
      " [0.09867077 1.0802162  1.5825598 ]\n",
      " [0.10471343 1.0875878  1.5858451 ]\n",
      " [0.1107582  1.093641   1.5868376 ]\n",
      " [0.11681731 1.0984082  1.5856148 ]\n",
      " [0.12290294 1.1019213  1.5822538 ]]\n"
     ]
    }
   ],
   "source": [
    "print('Deform[0:10,0,0]',deform[0:10,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-18 10:26:38 - asyncio - DEBUG - Using selector: KqueueSelector\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - State start\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO -   Scheduler at: tcp://192.168.1.219:64360\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO -   dashboard at:  http://192.168.1.219:8787/status\n",
      "2024-06-18 10:26:38 - distributed.scheduler - INFO - Registering Worker plugin shuffle\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.1.219:64363'\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.1.219:64367'\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.1.219:64365'\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.1.219:64369'\n",
      "2024-06-18 10:26:38 - distributed.nanny - INFO -         Start Nanny at: 'tcp://192.168.1.219:64371'\n",
      "2024-06-18 10:26:39 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.1.219:64373', name: 0, status: init, memory: 0, processing: 0>\n",
      "2024-06-18 10:26:39 - distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.1.219:64373\n",
      "2024-06-18 10:26:39 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64383\n",
      "2024-06-18 10:26:39 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.1.219:64375', name: 4, status: init, memory: 0, processing: 0>\n",
      "2024-06-18 10:26:39 - distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.1.219:64375\n",
      "2024-06-18 10:26:39 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64384\n",
      "2024-06-18 10:26:39 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.1.219:64376', name: 2, status: init, memory: 0, processing: 0>\n",
      "2024-06-18 10:26:39 - distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.1.219:64376\n",
      "2024-06-18 10:26:39 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64386\n",
      "2024-06-18 10:26:39 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.1.219:64374', name: 1, status: init, memory: 0, processing: 0>\n",
      "2024-06-18 10:26:39 - distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.1.219:64374\n",
      "2024-06-18 10:26:39 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64385\n",
      "2024-06-18 10:26:39 - distributed.scheduler - INFO - Register worker <WorkerState 'tcp://192.168.1.219:64377', name: 3, status: init, memory: 0, processing: 0>\n",
      "2024-06-18 10:26:39 - distributed.scheduler - INFO - Starting worker compute stream, tcp://192.168.1.219:64377\n",
      "2024-06-18 10:26:39 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64387\n",
      "2024-06-18 10:26:39 - distributed.scheduler - INFO - Receive client connection: Client-c6eca8fc-2d7e-11ef-a79b-fe4548c183a7\n",
      "2024-06-18 10:26:39 - distributed.core - INFO - Starting established connection to tcp://192.168.1.219:64388\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Remove client Client-c6eca8fc-2d7e-11ef-a79b-fe4548c183a7\n",
      "2024-06-18 10:26:41 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64388; closing.\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Remove client Client-c6eca8fc-2d7e-11ef-a79b-fe4548c183a7\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Close client connection: Client-c6eca8fc-2d7e-11ef-a79b-fe4548c183a7\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Retire worker addresses (0, 1, 2, 3, 4)\n",
      "2024-06-18 10:26:41 - distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.1.219:64363'. Reason: nanny-close\n",
      "2024-06-18 10:26:41 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2024-06-18 10:26:41 - distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.1.219:64365'. Reason: nanny-close\n",
      "2024-06-18 10:26:41 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2024-06-18 10:26:41 - distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.1.219:64367'. Reason: nanny-close\n",
      "2024-06-18 10:26:41 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2024-06-18 10:26:41 - distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.1.219:64369'. Reason: nanny-close\n",
      "2024-06-18 10:26:41 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2024-06-18 10:26:41 - distributed.nanny - INFO - Closing Nanny at 'tcp://192.168.1.219:64371'. Reason: nanny-close\n",
      "2024-06-18 10:26:41 - distributed.nanny - INFO - Nanny asking worker to close. Reason: nanny-close\n",
      "2024-06-18 10:26:41 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64383; closing.\n",
      "2024-06-18 10:26:41 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64385; closing.\n",
      "2024-06-18 10:26:41 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64386; closing.\n",
      "2024-06-18 10:26:41 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64387; closing.\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://192.168.1.219:64373', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718720801.101305')\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://192.168.1.219:64374', name: 1, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718720801.102706')\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://192.168.1.219:64376', name: 2, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718720801.104949')\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://192.168.1.219:64377', name: 3, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718720801.106716')\n",
      "2024-06-18 10:26:41 - distributed.core - INFO - Received 'close-stream' from tcp://192.168.1.219:64384; closing.\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://192.168.1.219:64375', name: 4, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1718720801.111145')\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Lost all workers\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Scheduler closing due to unknown reason...\n",
      "2024-06-18 10:26:41 - distributed.scheduler - INFO - Scheduler closing all comms\n",
      "Write aligned result to:/Users/goinac/Work/HHMI/bioimagetools/nextflow-modules/mylocal/results/bigstream-python/r2c0s0_aligned_to_r1.nrrd\n"
     ]
    }
   ],
   "source": [
    "from bigstream.piecewise_transform import distributed_apply_transform\n",
    "\n",
    "blocksize = (64,)*3\n",
    "channels = [ref_ch]\n",
    "\n",
    "fix = fix_zarr[f'/{ref_ch}/{local_scale}']\n",
    "for channel in channels:\n",
    "    mov = mov_zarr[f'/{channel}/{local_scale}']\n",
    "    aligned = distributed_apply_transform(\n",
    "        fix, mov,\n",
    "        fix_deform_spacing, mov_deform_spacing,\n",
    "        transform_list=[affine, deform,],\n",
    "        transform_spacing=((1,1,1), fix_deform_spacing),\n",
    "        blocksize=blocksize,\n",
    "        cluster_kwargs=cluster_kwargs\n",
    "    )\n",
    "    print(f'Write aligned result to:{out_p}/r2{channel}{local_scale}_aligned_to_r1.nrrd')\n",
    "    nrrd.write(out_p+f'/r2{channel}{local_scale}_aligned_to_r1.nrrd', aligned.transpose(2,1,0), compression_level=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigstream_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
